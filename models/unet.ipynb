{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "changing-driver",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "worse-cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from skimage import io\n",
    "# For everything\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# For our model\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms\n",
    "# For utilities\n",
    "import os, shutil, time\n",
    "import cv2 as cv\n",
    "import subprocess\n",
    "from torch.multiprocessing import Pool, set_start_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "secure-course",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "# remove .ipynb_chaeckpoint files\n",
    "subprocess.run('../rm_ipynbcheckpoints.sh', shell=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "straight-garden",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from https://amaarora.github.io/2020/09/13/unet.html#u-net\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv2(self.relu(self.conv1(x)))\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, chs=(8,64,128,256,512,1024)):\n",
    "        super().__init__()\n",
    "        self.enc_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n",
    "        self.pool       = nn.MaxPool2d(2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ftrs = []\n",
    "        for block in self.enc_blocks:\n",
    "            x = block(x)\n",
    "            ftrs.append(x)\n",
    "            x = self.pool(x)\n",
    "        return ftrs\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, chs=(1024, 512, 256, 128, 40)):\n",
    "        super().__init__()\n",
    "        self.chs         = chs\n",
    "        self.upconvs    = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i+1], 2, 2) for i in range(len(chs)-1)])\n",
    "        self.dec_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)]) \n",
    "        \n",
    "    def forward(self, x, encoder_features):\n",
    "        for i in range(len(self.chs)-1):\n",
    "            x        = self.upconvs[i](x)\n",
    "            enc_ftrs = self.crop(encoder_features[i], x)\n",
    "            x        = torch.cat([x, enc_ftrs], dim=1)            \n",
    "            x        = self.dec_blocks[i](x)\n",
    "        return x\n",
    "    \n",
    "    def crop(self, enc_ftrs, x):\n",
    "        _, _, H, W = x.shape\n",
    "        enc_ftrs   = transforms.CenterCrop([H, W])(enc_ftrs)\n",
    "        return enc_ftrs\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, enc_chs=(8,64,128,256,512,1024), dec_chs=(1024, 512, 256, 128, 64)):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(enc_chs)\n",
    "        self.decoder = Decoder(dec_chs)\n",
    "        self.head = nn.Conv2d(dec_chs[-1], 40, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_ftrs = self.encoder(x)\n",
    "        out = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])\n",
    "        out = self.head(out)\n",
    "        out = F.interpolate(out, (256, 256))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "associate-somewhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet().double()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "drawn-overall",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STPT_IMC_ImageFolder(datasets.ImageFolder):    \n",
    "    \"\"\"\n",
    "    Preprocesses\n",
    "    \"\"\"\n",
    "    def __init__(self, root, transform, bits=8, batch_size=64):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.imc_folder = os.path.join(self.root, 'IMC')\n",
    "        self.stpt_folder = os.path.join(self.root, 'STPT')\n",
    "        self.bits = bits # num bits for each pixel in image\n",
    "        self.batch_size = batch_size\n",
    "        if self.root == 'data/train':\n",
    "            self.index_to_phys_sec = [1, 2, 3, 5, 6, 7, 9, 10, 11, 13, 14, 15, 17, 18]\n",
    "        elif self.root == 'data/val':\n",
    "            self.index_to_phys_sec = [4, 8, 12]\n",
    "            \n",
    "    def __len__(self):\n",
    "        # length of dataset dictated by num aligned IMC images b/c len(IMC) < len(STPT)\n",
    "        return len(os.listdir(self.imc_folder))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        phys_sec = self.index_to_phys_sec[index]\n",
    "        # ====== GET IMAGE PATHS ======\n",
    "        # define folder paths for physical section defined by index\n",
    "        imc_section_folder = os.path.join(self.imc_folder,\n",
    "                                          'SECTION_{}'.format(str(phys_sec).zfill(2)))\n",
    "        \n",
    "        # get a list of all .tif images inside imc_section_folder\n",
    "        imc_img_paths = [os.path.join(imc_section_folder, imc_img_path)\n",
    "                         for imc_img_path in os.listdir(imc_section_folder)\n",
    "                         if imc_img_path.endswith('.tif')]\n",
    "        \n",
    "        # get path to stpt images\n",
    "        stpt_img_paths = [os.path.join(self.stpt_folder,\n",
    "                                       'S{0}_Z{1}.tif'.format(str(phys_sec).zfill(3),\n",
    "                                                          optical_section.zfill(2)))\n",
    "                          for optical_section in ['0', '1']] \n",
    "        \n",
    "        \n",
    "        # ====== LOAD IMAGES ======\n",
    "        with Pool(maxtasksperchild=100) as p:\n",
    "            imc_imgs = list(p.imap(self.process_imc_image, imc_img_paths))\n",
    "            stpt_imgs = list(p.imap(self.process_stpt_image, stpt_img_paths))\n",
    "            \n",
    "        # postprocess loaded images\n",
    "        imc_imgs = [torch.unsqueeze(img, 0) for img in imc_imgs] # add an extra dimesion for channel\n",
    "        imc_imgs_cat = torch.cat(imc_imgs, 0) # (40, 18720, 18720)\n",
    "        \n",
    "        stpt_imgs = [img.permute((2,0,1)) for img in stpt_imgs] # (C,H,W) tensor\n",
    "        stpt_imgs_cat = torch.cat(stpt_imgs, 0) # concatenate two stpt images (8, 20800, 20800)\n",
    "        \n",
    "        \n",
    "        # ====== TRANSFORMS ======\n",
    "        stpt_imgs_cat = transforms.Resize(imc_imgs[0].shape[1])(stpt_imgs_cat)  # make STPT img same size as IMC (..., 18720, 18720)\n",
    "        combine = torch.cat((imc_imgs_cat, stpt_imgs_cat), 0) # combine imc and stpt -> (48, 18720, 18720)\n",
    "        \n",
    "        # obtain a batch of random crops\n",
    "        img_set = [self.transform(combine) for i in range(self.batch_size)]\n",
    "            \n",
    "        # separate imc and stpt -> (40, 18720, 18720), (8, 18720, 18720)\n",
    "        imc_imgs = [torch.split(img, 40)[0] for img in img_set]\n",
    "        stpt_imgs = [torch.split(img, 40)[1] for img in img_set]\n",
    "        \n",
    "        return stpt_imgs, imc_imgs\n",
    "    \n",
    "    def process_stpt_image(self, file_name):\n",
    "        img = io.imread(file_name)\n",
    "        return torch.from_numpy(img.astype('uint8'))\n",
    "    \n",
    "    def process_imc_image(self, file_name):\n",
    "        # read image file\n",
    "        img = cv.imread(file_name, cv.IMREAD_UNCHANGED)\n",
    "\n",
    "        # normalize image\n",
    "        norm_img = img.copy()\n",
    "        cv.normalize(img, norm_img, alpha=0, beta=2**self.bits - 1, norm_type=cv.NORM_MINMAX)\n",
    "\n",
    "        # Apply log transformation method\n",
    "        c = (2**self.bits - 1) / np.log(1 + np.max(norm_img))\n",
    "        \n",
    "        log_image = c * (np.log(norm_img + 1))\n",
    "        \n",
    "        # Specify the data type so that\n",
    "        # float value will be converted to int\n",
    "        return torch.from_numpy(log_image.astype('uint8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "nasty-explorer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_cropped_images(batch):\n",
    "    \"\"\"\n",
    "    takes in a batch of cropped images from a single physical section and forms\n",
    "    a mini-batch for the DataLoader class\n",
    "    \"\"\"\n",
    "    stpt_imgs = batch[0][0]\n",
    "    imc_imgs = batch[0][1]\n",
    "    return torch.stack(stpt_imgs), torch.stack(imc_imgs)\n",
    "\n",
    "# Training\n",
    "train_transforms = transforms.Compose([transforms.RandomCrop(256)])\n",
    "train_imagefolder = STPT_IMC_ImageFolder(root='data/train',\n",
    "                                         transform=train_transforms)\n",
    "train_loader = torch.utils.data.DataLoader(train_imagefolder,\n",
    "                                           batch_size=1,\n",
    "                                           shuffle=True,\n",
    "                                           collate_fn=merge_cropped_images)\n",
    "\n",
    "# Validation \n",
    "val_transforms = transforms.Compose([transforms.RandomCrop(256)])\n",
    "val_imagefolder = STPT_IMC_ImageFolder(root='data/val',\n",
    "                                       transform=val_transforms)\n",
    "val_loader = torch.utils.data.DataLoader(val_imagefolder,\n",
    "                                         batch_size=1,\n",
    "                                         shuffle=False,\n",
    "                                         collate_fn=merge_cropped_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "manufactured-horse",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "  '''A handy class from the PyTorch ImageNet tutorial''' \n",
    "  def __init__(self):\n",
    "    self.reset()\n",
    "  def reset(self):\n",
    "    self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n",
    "  def update(self, val, n=1):\n",
    "    self.val = val\n",
    "    self.sum += val * n\n",
    "    self.count += n\n",
    "    self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "varying-replica",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, epoch):\n",
    "  print('='*10, 'Starting validation epoch {}'.format(epoch), '='*10) \n",
    "  model.eval()\n",
    "\n",
    "  # Prepare value counters and timers\n",
    "  batch_time, data_time, losses = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "\n",
    "  end = time.time()\n",
    "  already_saved_images = False\n",
    "  for i, (stpt, imc) in enumerate(val_loader):\n",
    "#     print('**Validation iteration {}**'.format(i))\n",
    "    data_time.update(time.time() - end)\n",
    "\n",
    "    # Use GPU\n",
    "    if use_gpu: \n",
    "        stpt, imc = stpt.cuda(), imc.cuda()\n",
    "\n",
    "    # Run model and record loss\n",
    "    imc_recons = model(stpt.double()).cuda() # throw away class predictions\n",
    "    loss = criterion(imc_recons.double(), imc.double())\n",
    "    losses.update(loss.item(), stpt.size(0))\n",
    "\n",
    "    # Record time to do forward passes and save images\n",
    "    batch_time.update(time.time() - end)\n",
    "    end = time.time()\n",
    "\n",
    "    # Print model accuracy -- in the code below, val refers to both value and validation\n",
    "    if i % 25 == 0:\n",
    "      print('Validate: [{0}/{1}]\\t'\n",
    "            'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "            'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
    "             i, len(val_loader), batch_time=batch_time, loss=losses))\n",
    "    \n",
    "  return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "automated-appearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "  print('='*10, 'Starting training epoch {}'.format(epoch), '='*10)\n",
    "  model.train()\n",
    "  \n",
    "  # Prepare value counters and timers\n",
    "  batch_time, data_time, losses = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "\n",
    "  end = time.time()\n",
    "  for i, (stpt, imc) in enumerate(train_loader):\n",
    "#     print('**Training iteration {}**'.format(i))\n",
    "    \n",
    "    # Use GPU if available\n",
    "    if use_gpu:\n",
    "        stpt, imc = stpt.cuda(), imc.cuda()\n",
    "\n",
    "    # Record time to load data (above)\n",
    "    data_time.update(time.time() - end)\n",
    "\n",
    "    # Run forward pass\n",
    "    imc_recons = model(stpt.double()).cuda()\n",
    "    loss = criterion(imc_recons.double(), imc.double()) \n",
    "    losses.update(loss.item(), stpt.size(0))\n",
    "\n",
    "    # Compute gradient and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Record time to do forward and backward passes\n",
    "    batch_time.update(time.time() - end)\n",
    "    end = time.time()\n",
    "\n",
    "    # Print model accuracy -- in the code below, val refers to value, not validation\n",
    "    if i % 25 == 0:\n",
    "      print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "            'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "            'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "            'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
    "              epoch, i, len(train_loader), batch_time=batch_time,\n",
    "             data_time=data_time, loss=losses)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cross-richmond",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model and loss function to GPU\n",
    "if use_gpu: \n",
    "    criterion = criterion.cuda()\n",
    "    model = model.cuda()\n",
    "\n",
    "model.share_memory();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "polar-tokyo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Starting training epoch 0 ==========\n",
      "Epoch: [0][0/14]\tTime 66.384 (66.384)\tData 62.371 (62.371)\tLoss 702.0359 (702.0359)\t\n",
      "========== Finished training epoch 0 ==========\n",
      "========== Starting validation epoch 0 ==========\n",
      "Validate: [0/3]\tTime 69.954 (69.954)\tLoss 566493896966496125558033852923904.0000 (566493896966496125558033852923904.0000)\t\n",
      "Finished validation.\n",
      "========== Starting training epoch 1 ==========\n",
      "Epoch: [1][0/14]\tTime 70.599 (70.599)\tData 66.686 (66.686)\tLoss 560952209038942663518587558297600.0000 (560952209038942663518587558297600.0000)\t\n",
      "========== Finished training epoch 1 ==========\n",
      "========== Starting validation epoch 1 ==========\n",
      "Validate: [0/3]\tTime 71.580 (71.580)\tLoss 1141.1974 (1141.1974)\t\n",
      "Finished validation.\n",
      "========== Starting training epoch 2 ==========\n",
      "Epoch: [2][0/14]\tTime 70.271 (70.271)\tData 66.355 (66.355)\tLoss 450.0416 (450.0416)\t\n",
      "========== Finished training epoch 2 ==========\n",
      "========== Starting validation epoch 2 ==========\n",
      "Validate: [0/3]\tTime 72.311 (72.311)\tLoss 4502.4986 (4502.4986)\t\n",
      "Finished validation.\n",
      "========== Starting training epoch 3 ==========\n",
      "Epoch: [3][0/14]\tTime 80.112 (80.112)\tData 76.199 (76.199)\tLoss 3946.2134 (3946.2134)\t\n",
      "========== Finished training epoch 3 ==========\n",
      "========== Starting validation epoch 3 ==========\n",
      "Validate: [0/3]\tTime 73.023 (73.023)\tLoss 8573.7977 (8573.7977)\t\n",
      "Finished validation.\n",
      "========== Starting training epoch 4 ==========\n",
      "Epoch: [4][0/14]\tTime 71.062 (71.062)\tData 67.149 (67.149)\tLoss 4261.2223 (4261.2223)\t\n",
      "========== Finished training epoch 4 ==========\n",
      "========== Starting validation epoch 4 ==========\n",
      "Validate: [0/3]\tTime 70.831 (70.831)\tLoss 7046.7484 (7046.7484)\t\n",
      "Finished validation.\n",
      "========== Starting training epoch 5 ==========\n",
      "Epoch: [5][0/14]\tTime 68.523 (68.523)\tData 64.609 (64.609)\tLoss 8664.1829 (8664.1829)\t\n",
      "========== Finished training epoch 5 ==========\n",
      "========== Starting validation epoch 5 ==========\n",
      "Validate: [0/3]\tTime 71.482 (71.482)\tLoss 9900.8050 (9900.8050)\t\n",
      "Finished validation.\n",
      "========== Starting training epoch 6 ==========\n",
      "Epoch: [6][0/14]\tTime 72.172 (72.172)\tData 68.256 (68.256)\tLoss 7619.5736 (7619.5736)\t\n",
      "========== Finished training epoch 6 ==========\n",
      "========== Starting validation epoch 6 ==========\n",
      "Validate: [0/3]\tTime 67.200 (67.200)\tLoss 9531.8796 (9531.8796)\t\n",
      "Finished validation.\n",
      "========== Starting training epoch 7 ==========\n",
      "Epoch: [7][0/14]\tTime 68.971 (68.971)\tData 65.052 (65.052)\tLoss 8038.0388 (8038.0388)\t\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/train/IMC/SECTION_02'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e10165999c74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0;31m# Train for one epoch, then validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m       \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-9ebe0baffe58>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m#     print('**Training iteration {}**'.format(i))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-e2a278f0c716>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# get a list of all .tif images inside imc_section_folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         imc_img_paths = [os.path.join(imc_section_folder, imc_img_path)\n\u001b[0;32m---> 31\u001b[0;31m                          \u001b[0;32mfor\u001b[0m \u001b[0mimc_img_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimc_section_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                          if imc_img_path.endswith('.tif')]\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/train/IMC/SECTION_02'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    best_losses = 1e10\n",
    "    epochs = 20\n",
    "\n",
    "    # Train model\n",
    "    for epoch in range(epochs):\n",
    "      # Train for one epoch, then validate\n",
    "      train(train_loader, model, criterion, optimizer, epoch)\n",
    "      with torch.no_grad():\n",
    "        losses = validate(val_loader, model, criterion, epoch)\n",
    "      # Save checkpoint and replace old best model if current model is better\n",
    "      if losses < best_losses:\n",
    "        best_losses = losses\n",
    "        torch.save(model.state_dict(), 'checkpoints/model-epoch-{}-losses-{:.3f}.pth'.format(epoch+1,losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-presentation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpsc533y",
   "language": "python",
   "name": "cpsc533y"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
