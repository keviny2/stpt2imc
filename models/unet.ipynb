{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "liquid-amount",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fundamental-surprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from skimage import io\n",
    "# For everything\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# For our model\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms\n",
    "# For utilities\n",
    "import os, shutil, time\n",
    "import cv2 as cv\n",
    "import subprocess\n",
    "from torch.multiprocessing import Pool, set_start_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "regulation-helen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "# remove .ipynb_chaeckpoint files\n",
    "subprocess.run('.././rm_ipynbcheckpoints.sh', shell=True, cwd='/home/kyang/Shared/Notebooks/Kevin/stpt2imc');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "plastic-geometry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from https://amaarora.github.io/2020/09/13/unet.html#u-net\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3)\n",
    "        self.batchnorm2d = nn.BatchNorm2d(in_ch)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batchnorm2d(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        return self.conv2(x)\n",
    "#         return self.conv2(self.relu(self.conv1(self.batchnorm2d(x))))\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, chs=(8,64,128,256,512,1024)):\n",
    "        super().__init__()\n",
    "        self.enc_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n",
    "        self.pool       = nn.MaxPool2d(2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ftrs = []\n",
    "        for block in self.enc_blocks:\n",
    "            x = block(x)\n",
    "            ftrs.append(x)\n",
    "            x = self.pool(x)\n",
    "        return ftrs\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, chs=(1024, 512, 256, 128, 40)):\n",
    "        super().__init__()\n",
    "        self.chs         = chs\n",
    "        self.upconvs    = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i+1], 2, 2) for i in range(len(chs)-1)])\n",
    "        self.dec_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)]) \n",
    "        \n",
    "    def forward(self, x, encoder_features):\n",
    "        for i in range(len(self.chs)-1):\n",
    "            x        = self.upconvs[i](x)\n",
    "            enc_ftrs = self.crop(encoder_features[i], x)\n",
    "            x        = torch.cat([x, enc_ftrs], dim=1)            \n",
    "            x        = self.dec_blocks[i](x)\n",
    "        return x\n",
    "    \n",
    "    def crop(self, enc_ftrs, x):\n",
    "        _, _, H, W = x.shape\n",
    "        enc_ftrs   = transforms.CenterCrop([H, W])(enc_ftrs)\n",
    "        return enc_ftrs\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, enc_chs=(8,64,128,256,512,1024), dec_chs=(1024, 512, 256, 128, 64)):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(enc_chs)\n",
    "        self.decoder = Decoder(dec_chs)\n",
    "        self.head = nn.Conv2d(dec_chs[-1], 40, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_ftrs = self.encoder(x)\n",
    "        out = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])\n",
    "        out = self.head(out)\n",
    "        out = F.interpolate(out, (256, 256))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "attended-character",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet().double()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Move model and loss function to GPU\n",
    "if use_gpu: \n",
    "    criterion = criterion.cuda()\n",
    "    model = model.cuda()\n",
    "\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.1, weight_decay=0.01)\n",
    "\n",
    "# https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training\n",
    "\n",
    "checkpoint = torch.load('../checkpoints/model-epoch-7-losses-282.914.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "inc = checkpoint['epoch'] + 1 # increment depending on how many epochs we already completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "drawn-nicaragua",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STPT_IMC_ImageFolder(datasets.ImageFolder):    \n",
    "    \"\"\"\n",
    "    Preprocesses\n",
    "    \"\"\"\n",
    "    def __init__(self, root, transform, bits=8, batch_size=64):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.imc_folder = os.path.join(self.root, 'IMC')\n",
    "        self.stpt_folder = os.path.join(self.root, 'STPT')\n",
    "        self.bits = bits # num bits for each pixel in image\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # length of dataset will be the total number of files contained in all subdirectories inside self.imc_folder\n",
    "        self.num_imgs_per_phys_sec = len(os.listdir(os.path.join(self.imc_folder, '01')))\n",
    "        self.num_imgs = self.num_imgs_per_phys_sec * 15  # 15 physical sections\n",
    "        \n",
    "        self.index_to_phys_sec = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]  # skip phys_sec 16\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.num_imgs\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        phys_sec = self.index_to_phys_sec[int(np.floor(index / self.num_imgs_per_phys_sec))]  # mod to find physical section\n",
    "                                                         \n",
    "        # ====== GET LIST OF IMAGE FILES ======\n",
    "        stpt_imgs = os.listdir(os.path.join(self.stpt_folder,\n",
    "                                    '{}'.format(str(phys_sec).zfill(2)))) \n",
    "                                                         \n",
    "        imc_imgs = os.listdir(os.path.join(self.imc_folder,\n",
    "                                           '{}'.format(str(phys_sec).zfill(2))))\n",
    "        \n",
    "        # ====== GET IMAGE FILE PATH ======\n",
    "        stpt_path = os.path.join(self.stpt_folder,\n",
    "                                           '{}'.format(str(phys_sec).zfill(2)),\n",
    "                                           stpt_imgs[int(index % self.num_imgs_per_phys_sec)])\n",
    "        \n",
    "        imc_path = os.path.join(self.imc_folder,\n",
    "                                          '{}'.format(str(phys_sec).zfill(2)),\n",
    "                                          imc_imgs[int(index % self.num_imgs_per_phys_sec)])\n",
    "\n",
    "        # make sure the files line up\n",
    "        try:\n",
    "            assert(os.path.basename(stpt_path) == os.path.basename(imc_path))\n",
    "        except:\n",
    "            print('stpt path:', os.path.basename(stpt_path))\n",
    "            print('imc path:', os.path.basename(imc_path))\n",
    "                                       \n",
    "        # ====== LOAD IMAGES ======\n",
    "        stpt_img = torch.load(stpt_path)\n",
    "\n",
    "        imc_img = torch.load(imc_path)\n",
    "                                                                     \n",
    "        return stpt_img, imc_img   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sophisticated-effectiveness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "# not using transforms\n",
    "transform = None\n",
    "\n",
    "train_imagefolder = STPT_IMC_ImageFolder(root='../data/train',\n",
    "                                         transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_imagefolder,\n",
    "                                           batch_size=64,\n",
    "                                           shuffle=True)\n",
    "\n",
    "# Validation \n",
    "# val_transforms = transforms.Compose([transforms.Normalize(normalize_param, normalize_param)])\n",
    "val_imagefolder = STPT_IMC_ImageFolder(root='../data/val',\n",
    "                                       transform=transform)\n",
    "val_loader = torch.utils.data.DataLoader(val_imagefolder,\n",
    "                                         batch_size=64,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "appointed-packet",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "  '''A handy class from the PyTorch ImageNet tutorial''' \n",
    "  def __init__(self):\n",
    "    self.reset()\n",
    "    self.vals = []\n",
    "    self.avgs = []\n",
    "  def reset(self):\n",
    "    self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n",
    "  def update(self, val, n=1):\n",
    "    self.val = val\n",
    "    self.sum += val * n\n",
    "    self.count += n\n",
    "    self.avg = self.sum / self.count\n",
    "    self.vals.append(self.val)\n",
    "    self.avgs.append(self.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "suspended-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, epoch, plot=True):\n",
    "  print('='*10, 'Starting validation epoch {}'.format(epoch), '='*10) \n",
    "  model.eval()\n",
    "\n",
    "  # Prepare value counters and timers\n",
    "  batch_time, data_time, losses = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "\n",
    "  end = time.time()\n",
    "  already_saved_images = False\n",
    "  for i, (stpt, imc) in enumerate(val_loader):\n",
    "    data_time.update(time.time() - end)\n",
    "\n",
    "    # Use GPU\n",
    "    if use_gpu: \n",
    "        stpt, imc = stpt.cuda(), imc.cuda()\n",
    "\n",
    "    # Run model and record loss\n",
    "    imc_recons = model(stpt.double()).cuda() # throw away class predictions\n",
    "    loss = criterion(imc_recons.double(), imc.double())\n",
    "    losses.update(loss.item(), stpt.size(0))\n",
    "\n",
    "    # Record time to do forward passes and save images\n",
    "    batch_time.update(time.time() - end)\n",
    "    end = time.time()\n",
    "\n",
    "    # Print model accuracy -- in the code below, val refers to both value and validation\n",
    "    if i % 25 == 0:\n",
    "      print('Validate: [{0}/{1}]\\t'\n",
    "            'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "            'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
    "             i, len(val_loader), batch_time=batch_time, loss=losses))\n",
    "    \n",
    "  return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "classical-convert",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, plot=True):\n",
    "  print('='*10, 'Starting training epoch {}'.format(epoch), '='*10)\n",
    "  model.train()\n",
    "  \n",
    "  # Prepare value counters and timers\n",
    "  batch_time, data_time, losses = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "\n",
    "  end = time.time()\n",
    "  for i, (stpt, imc) in enumerate(train_loader):\n",
    "    \n",
    "    # Use GPU if available\n",
    "    if use_gpu:\n",
    "        stpt, imc = stpt.cuda(), imc.cuda()\n",
    "\n",
    "    # Record time to load data (above)\n",
    "    data_time.update(time.time() - end)\n",
    "\n",
    "    # Run forward pass\n",
    "    imc_recons = model(stpt.double()).cuda()\n",
    "    loss = criterion(imc_recons.double(), imc.double()) \n",
    "    losses.update(loss.item(), stpt.size(0))\n",
    "\n",
    "    # Compute gradient and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Record time to do forward and backward passes\n",
    "    batch_time.update(time.time() - end)\n",
    "    end = time.time()\n",
    "\n",
    "    # Print model accuracy -- in the code below, val refers to value, not validation\n",
    "    if i % 25 == 0:\n",
    "      print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "            'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "            'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "            'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
    "              epoch, i, len(train_loader), batch_time=batch_time,\n",
    "             data_time=data_time, loss=losses)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-disaster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Starting training epoch 7 ==========\n",
      "Epoch: [7][0/1125]\tTime 11.514 (11.514)\tData 7.681 (7.681)\tLoss 311.0457 (311.0457)\t\n",
      "Epoch: [7][25/1125]\tTime 9.374 (9.579)\tData 5.569 (5.772)\tLoss 303.8727 (279.2831)\t\n",
      "Epoch: [7][50/1125]\tTime 9.361 (9.615)\tData 5.554 (5.810)\tLoss 286.9803 (281.5090)\t\n",
      "Epoch: [7][75/1125]\tTime 9.894 (9.625)\tData 6.089 (5.820)\tLoss 259.1132 (280.1500)\t\n",
      "Epoch: [7][100/1125]\tTime 9.913 (9.672)\tData 6.109 (5.867)\tLoss 298.8916 (281.5730)\t\n",
      "Epoch: [7][125/1125]\tTime 9.345 (9.648)\tData 5.532 (5.842)\tLoss 206.5334 (277.5395)\t\n",
      "Epoch: [7][150/1125]\tTime 9.777 (9.644)\tData 5.973 (5.839)\tLoss 334.1176 (277.9638)\t\n",
      "Epoch: [7][175/1125]\tTime 9.794 (9.683)\tData 5.989 (5.877)\tLoss 288.3357 (277.9542)\t\n",
      "Epoch: [7][200/1125]\tTime 9.728 (9.700)\tData 5.918 (5.895)\tLoss 245.4786 (277.4311)\t\n",
      "Epoch: [7][225/1125]\tTime 10.526 (9.726)\tData 6.719 (5.921)\tLoss 220.3418 (276.3437)\t\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    best_losses = 1e10\n",
    "    epochs = 20\n",
    "\n",
    "    # Train model\n",
    "    for epoch in range(epochs):\n",
    "      epoch += inc\n",
    "      # Train for one epoch, then validate\n",
    "      train(train_loader, model, criterion, optimizer, epoch)\n",
    "      with torch.no_grad():\n",
    "        losses = validate(val_loader, model, criterion, epoch)\n",
    "      # Save checkpoint and replace old best model if current model is better\n",
    "      if losses < best_losses:\n",
    "        best_losses = losses\n",
    "        torch.save({'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': losses,\n",
    "                    'epoch': epoch,\n",
    "                    'loss': losses\n",
    "                   }, '../checkpoints/model-epoch-{}-losses-{:.3f}.pth'.format(epoch+1,losses))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpsc533y",
   "language": "python",
   "name": "cpsc533y"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
