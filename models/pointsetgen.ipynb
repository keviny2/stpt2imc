{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "choice-settle",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "conditional-mortality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from skimage import io\n",
    "# For everything\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# For our model\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms\n",
    "# For utilities\n",
    "import os, shutil, time\n",
    "import cv2 as cv\n",
    "import subprocess\n",
    "from torch.multiprocessing import Pool, set_start_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "challenging-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "# remove .ipynb_chaeckpoint files\n",
    "subprocess.run('.././rm_ipynbcheckpoints.sh', shell=True, cwd='/home/kyang/Shared/Notebooks/Kevin/stpt2imc');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "renewable-tractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block3(nn.Module):\n",
    "    '''\n",
    "    Module consisting of 3 convolutional layers\n",
    "    '''\n",
    "    def __init__(self, in_ch, out_ch, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=kernel_size, stride=2),  # first stride is always 2\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            \n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3),  # constant kernel size from here\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            \n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(out_ch)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    \n",
    "class PointSetGen(nn.Module):\n",
    "    def __init__(self, in_ch=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # ====== ENCODER 1 ======\n",
    "        \n",
    "        self.beginning = nn.Sequential(\n",
    "            nn.BatchNorm2d(8),\n",
    "            \n",
    "            nn.Conv2d(8, 16, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            \n",
    "            nn.Conv2d(16, 16, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16)\n",
    "        )\n",
    "        \n",
    "        self.block3_1 = Block3(16, 32)\n",
    "        self.block3_2 = Block3(32, 64)\n",
    "        self.block3_3 = Block3(64, 128)\n",
    "        self.block3_4 = Block3(128, 256, kernel_size=5)\n",
    "        self.upblock = nn.Sequential(nn.Conv2d(256, 512, kernel_size=1))\n",
    "        \n",
    "        # ====== DECODER 1 ======\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(512)\n",
    "        )\n",
    "        self.fully_connected1 = nn.Sequential(\n",
    "            nn.Flatten(-2, -1),\n",
    "            nn.Linear(4, 2048),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.deconv1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=5),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "        \n",
    "        self.skip1 = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3),\n",
    "            nn.Upsample((12, 12))\n",
    "        )\n",
    "        self.comb1 = nn.Conv2d(256, 256, kernel_size=3)\n",
    "        self.blue1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=5),\n",
    "            nn.Upsample(scale_factor=2)\n",
    "        )\n",
    "        \n",
    "        self.skip2 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=3),\n",
    "            nn.Upsample((28, 28))\n",
    "        )\n",
    "        self.comb2 = nn.Conv2d(128, 128, kernel_size=3)\n",
    "        self.blue2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5),\n",
    "            nn.Upsample(scale_factor=2)\n",
    "        )\n",
    "        \n",
    "        self.skip3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=3),\n",
    "            nn.Upsample((60, 60))\n",
    "        )\n",
    "        self.comb3 = nn.Conv2d(64, 64, kernel_size=3)\n",
    "        self.blue3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=5),\n",
    "            nn.Upsample(scale_factor=2)\n",
    "        )   \n",
    "        \n",
    "        self.skip4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, kernel_size=3),\n",
    "            nn.Upsample((124, 124))\n",
    "        )\n",
    "        self.comb4 = nn.Conv2d(32, 32, kernel_size=3)\n",
    "        self.blue4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=5),\n",
    "            nn.Upsample(scale_factor=2)\n",
    "        )  \n",
    "        \n",
    "        self.skip5 = nn.Sequential(\n",
    "            nn.Conv2d(32, 16, kernel_size=3),\n",
    "            nn.Upsample((252, 252))\n",
    "        )\n",
    "        self.comb5 = nn.Sequential(\n",
    "            nn.Conv2d(16, 16, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2)\n",
    "        )\n",
    "        \n",
    "        # ====== ENCODER 2 ======\n",
    "        \n",
    "        self.enc_skip1 = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, kernel_size=3),\n",
    "            nn.Upsample((124, 124))\n",
    "        )\n",
    "        self.enc_comb1 = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.enc_skip2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3),\n",
    "            nn.Upsample((60, 60))\n",
    "        )\n",
    "        self.enc_comb2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "        self.enc_last2 = nn.Conv2d(64, 128, kernel_size=5, stride=2)\n",
    "        \n",
    "        self.enc_skip3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3),\n",
    "            nn.Upsample((27, 27))\n",
    "        )\n",
    "        self.enc_comb3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "        self.enc_last3 = nn.Conv2d(128, 256, kernel_size=5, stride=2)  \n",
    "        \n",
    "        self.enc_skip4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3),\n",
    "            nn.Upsample((11, 11))\n",
    "        )\n",
    "        self.enc_comb4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "        self.enc_last4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(512)\n",
    "        )\n",
    "        \n",
    "        # ====== PREDICTOR ======\n",
    "\n",
    "        self.fully_connected2 = nn.Linear(2048, 2048)\n",
    "        self.fully_connected3 = nn.Sequential(\n",
    "            nn.Flatten(-2, -1),\n",
    "            nn.Linear(9, 2048)\n",
    "        )\n",
    "\n",
    "        self.dec_blue1 = nn.ConvTranspose2d(512, 256, kernel_size=5, stride=2)\n",
    "        self.dec_skip1 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3),\n",
    "            nn.Upsample((9, 9))\n",
    "        )\n",
    "        self.convdeconv1 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=5, stride=2),\n",
    "            nn.Upsample(scale_factor=2)\n",
    "        )\n",
    "\n",
    "        self.dec_skip2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3),\n",
    "            nn.Upsample((34, 34))\n",
    "        )\n",
    "        self.convdeconv2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
    "            nn.Upsample(scale_factor=2)\n",
    "        )\n",
    "        \n",
    "        self.dec_skip3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3),\n",
    "            nn.Upsample((134, 134))\n",
    "        )\n",
    "        self.convdeconv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "        \n",
    "        self.fully_connected4 = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(1024, 600)\n",
    "#             nn.Linear(1024, 49152)\n",
    "        )\n",
    "        \n",
    "        self.finalconv_full = nn.Conv2d(512, 64, kernel_size=1)\n",
    "        self.finalconv_deconv = nn.Sequential(\n",
    "#             nn.Conv2d(64, 512, kernel_size=3),\n",
    "            nn.Upsample((45, 45))\n",
    "#             nn.Upsample((192, 192))\n",
    "        )\n",
    "        \n",
    "        self.mlp = nn.Conv2d(64, 40, kernel_size=1)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # ====== ENCODER 1 ======\n",
    "        \n",
    "        x = self.beginning(x)\n",
    "        \n",
    "        x = self.block3_1(x)\n",
    "        x1 = x    # can do this because torch returns new tensors for operations like nn.Conv2d\n",
    "        \n",
    "        # sequence of blocks of 3 convolutional layers\n",
    "        x = self.block3_2(x) \n",
    "        x2 = x\n",
    "        x = self.block3_3(x) \n",
    "        x3 = x\n",
    "        x = self.block3_4(x) \n",
    "        x4 = x\n",
    "\n",
    "        # substitute for block of 4 conv. layers b/c convolutions make images too small\n",
    "        x = self.upblock(x)\n",
    "        x5 = x\n",
    "        \n",
    "        # ====== DECODER 1 ======\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x_additional = self.fully_connected1(x)  # save for fully connected layer\n",
    "        x = self.deconv1(x)\n",
    "        \n",
    "        x5 = self.skip1(x5)\n",
    "        x = self.relu(torch.add(x, x5))  # torch.Size([1, 256, 12, 12])\n",
    "        x = self.relu(self.comb1(x))\n",
    "        x5 = x\n",
    "        x = self.blue1(x)\n",
    "        \n",
    "        x4 = self.skip2(x4)\n",
    "        x = self.relu(torch.add(x, x4))\n",
    "        x = self.relu(self.comb2(x))\n",
    "        x4 = x\n",
    "        x = self.blue2(x)\n",
    "        \n",
    "        x3 = self.skip3(x3)\n",
    "        x = self.relu(torch.add(x, x3))\n",
    "        x = self.relu(self.comb3(x))\n",
    "        x3 = x\n",
    "        x = self.blue3(x)\n",
    "\n",
    "        x2 = self.skip4(x2)\n",
    "        x = self.relu(torch.add(x, x2))\n",
    "        x = self.relu(self.comb4(x))\n",
    "        x2 = x\n",
    "        x = self.blue4(x)   \n",
    "        \n",
    "        x1 = self.skip5(x1)\n",
    "        x = self.relu(torch.add(x, x1))\n",
    "        x = self.comb5(x)\n",
    "        \n",
    "        # ====== ENCODER 2 ======\n",
    "        # the function name and variable names should be off by 1\n",
    "        x2 = self.enc_skip1(x2)\n",
    "        x = self.relu(torch.add(x, x2))\n",
    "        x = self.enc_comb1(x)\n",
    "        \n",
    "        x3 = self.enc_skip2(x3)\n",
    "        x = self.relu(torch.add(x, x3))\n",
    "        x = self.enc_comb2(x)\n",
    "        x3 = x\n",
    "        x = self.enc_last2(x)\n",
    "        \n",
    "        x4 = self.enc_skip3(x4)\n",
    "        x = self.relu(torch.add(x, x4))\n",
    "        x = self.enc_comb3(x)\n",
    "        x4 = x\n",
    "        x = self.enc_last3(x)\n",
    "        \n",
    "        x5 = self.enc_skip4(x5)\n",
    "        x = self.relu(torch.add(x, x5))\n",
    "        x = self.enc_comb4(x)\n",
    "        x5 = x\n",
    "        x = self.enc_last4(x)\n",
    "        \n",
    "        # ====== PREDICTOR ======\n",
    "        \n",
    "        x_additional = self.fully_connected2(x_additional)\n",
    "        x_additional = self.relu(torch.add(x_additional, self.fully_connected3(x)))\n",
    "        \n",
    "        x = self.dec_blue1(x)\n",
    "        x5 = self.dec_skip1(x5)\n",
    "        x = self.relu(torch.add(x, x5))\n",
    "        x = self.convdeconv1(x)\n",
    "        \n",
    "        x4 = self.dec_skip2(x4)\n",
    "        x = self.relu(torch.add(x, x4))\n",
    "        x = self.convdeconv2(x)\n",
    "        \n",
    "        x3 = self.dec_skip3(x3)\n",
    "        x = self.relu(torch.add(x, x3))\n",
    "        x = self.convdeconv3(x)\n",
    "        \n",
    "        x_additional = self.fully_connected4(x_additional) # torch.Size([1, 512, 600])\n",
    "        x_additional = torch.reshape(x_additional, (1, 512, 200, 3))\n",
    "        x_additional = self.finalconv_full(x_additional)\n",
    "        x = self.finalconv_deconv(x)\n",
    "        x = torch.reshape(x, (1, 64, 675, 3))\n",
    "        x = torch.cat((x_additional, x), 2)\n",
    "    \n",
    "        uv = torch.meshgrid(torch.arange(0, 256), torch.arange(0, 256))\n",
    "        uv = torch.stack(uv).permute(1,2,0).type(torch.uint8).cuda()  # [256, 256, 2]\n",
    "        xy = torch.sum(x.squeeze().type(torch.float32), dim=0) # [40, 875, 3]\n",
    "        \n",
    "#         curr = (uv[None,:,:,0]-xy[:,None,None,0])**2\n",
    "#         print('part 1')\n",
    "#         curr += (uv[None,:,:,1]-xy[:,None,None,1])**2\n",
    "#         print('part 2')\n",
    "#         curr /= (xy[:,None,None,2]**2 + 1)\n",
    "#         print('part 3')\n",
    "#         img = torch.exp(curr)\n",
    "#         print('yay!')\n",
    "\n",
    "        img = torch.exp(((uv[None,:,:,0]-xy[:,None,None,0])**2 + (uv[None,:,:,1]-xy[:,None,None,1])**2) / (xy[:,None,None,2]**2 + 1))  # [875,256,256]\n",
    "        x = self.mlp(x).squeeze()\n",
    "        x = torch.sum(x, dim=-1)\n",
    "        \n",
    "        x = x[:,:,None,None] * img[None,:,:,:]\n",
    "        x = torch.sum(x, dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "enhanced-expense",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PointSetGen().double()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Move model and loss function to GPU\n",
    "if use_gpu: \n",
    "    criterion = criterion.cuda()\n",
    "    model = model.cuda()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "# https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training\n",
    "\n",
    "# checkpoint = torch.load('../checkpoints/model-epoch-5-losses-285.453.pth')\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# inc = checkpoint['epoch'] + 1 # increment depending on how many epochs we already completed\n",
    "inc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "swiss-minister",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STPT_IMC_ImageFolder(datasets.ImageFolder):    \n",
    "    \"\"\"\n",
    "    Preprocesses\n",
    "    \"\"\"\n",
    "    def __init__(self, root, transform, bits=8, batch_size=64):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.imc_folder = os.path.join(self.root, 'IMC')\n",
    "        self.stpt_folder = os.path.join(self.root, 'STPT')\n",
    "        self.bits = bits # num bits for each pixel in image\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # length of dataset will be the total number of files contained in all subdirectories inside self.imc_folder\n",
    "        self.num_imgs_per_phys_sec = len(os.listdir(os.path.join(self.imc_folder, '01')))\n",
    "        self.num_imgs = self.num_imgs_per_phys_sec * 15  # 15 physical sections\n",
    "        \n",
    "        self.index_to_phys_sec = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]  # skip phys_sec 16\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.num_imgs\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        phys_sec = self.index_to_phys_sec[int(np.floor(index / self.num_imgs_per_phys_sec))]  # mod to find physical section\n",
    "                                                         \n",
    "        # ====== GET LIST OF IMAGE FILES ======\n",
    "        stpt_imgs = os.listdir(os.path.join(self.stpt_folder,\n",
    "                                    '{}'.format(str(phys_sec).zfill(2)))) \n",
    "                                                         \n",
    "        imc_imgs = os.listdir(os.path.join(self.imc_folder,\n",
    "                                           '{}'.format(str(phys_sec).zfill(2))))\n",
    "        \n",
    "        # ====== GET IMAGE FILE PATH ======\n",
    "        stpt_path = os.path.join(self.stpt_folder,\n",
    "                                           '{}'.format(str(phys_sec).zfill(2)),\n",
    "                                           stpt_imgs[int(index % self.num_imgs_per_phys_sec)])\n",
    "        \n",
    "        imc_path = os.path.join(self.imc_folder,\n",
    "                                          '{}'.format(str(phys_sec).zfill(2)),\n",
    "                                          imc_imgs[int(index % self.num_imgs_per_phys_sec)])\n",
    "\n",
    "        # make sure the files line up\n",
    "        try:\n",
    "            assert(os.path.basename(stpt_path) == os.path.basename(imc_path))\n",
    "        except:\n",
    "            print('stpt path:', os.path.basename(stpt_path))\n",
    "            print('imc path:', os.path.basename(imc_path))\n",
    "                                       \n",
    "        # ====== LOAD IMAGES ======\n",
    "#         stpt_img = self.transform[0](torch.load(stpt_path))  \n",
    "        stpt_img = torch.load(stpt_path)\n",
    "\n",
    "#         imc_img = self.transform[1](torch.load(imc_path))\n",
    "        imc_img = torch.load(imc_path)     \n",
    "                                                                     \n",
    "        return stpt_img, imc_img   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "clear-clause",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "stpt_normalize_param = [0.5 for i in range(8)]\n",
    "imc_normalize_param = [0.5 for i in range(40)]\n",
    "transform = [transforms.Normalize(stpt_normalize_param, stpt_normalize_param),\n",
    "              transforms.Normalize(imc_normalize_param, imc_normalize_param)]\n",
    "\n",
    "train_imagefolder = STPT_IMC_ImageFolder(root='../data/train',\n",
    "                                         transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_imagefolder,\n",
    "                                           batch_size=1,\n",
    "                                           shuffle=True)\n",
    "\n",
    "# Validation \n",
    "# val_transforms = transforms.Compose([transforms.Normalize(normalize_param, normalize_param)])\n",
    "val_imagefolder = STPT_IMC_ImageFolder(root='../data/val',\n",
    "                                       transform=transform)\n",
    "val_loader = torch.utils.data.DataLoader(val_imagefolder,\n",
    "                                         batch_size=1,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baking-landing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "  '''A handy class from the PyTorch ImageNet tutorial''' \n",
    "  def __init__(self):\n",
    "    self.reset()\n",
    "    self.vals = []\n",
    "    self.avgs = []\n",
    "  def reset(self):\n",
    "    self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n",
    "  def update(self, val, n=1):\n",
    "    self.val = val\n",
    "    self.sum += val * n\n",
    "    self.count += n\n",
    "    self.avg = self.sum / self.count\n",
    "    self.vals.append(self.val)\n",
    "    self.avgs.append(self.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "developing-child",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, epoch, plot=True):\n",
    "  print('='*10, 'Starting validation epoch {}'.format(epoch), '='*10) \n",
    "  model.eval()\n",
    "\n",
    "  # Prepare value counters and timers\n",
    "  batch_time, data_time, losses = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "\n",
    "  end = time.time()\n",
    "  already_saved_images = False\n",
    "  for i, (stpt, imc) in enumerate(val_loader):\n",
    "    data_time.update(time.time() - end)\n",
    "\n",
    "    # Use GPU\n",
    "    if use_gpu: \n",
    "        stpt, imc = stpt.cuda(), imc.cuda()\n",
    "\n",
    "    # Run model and record loss\n",
    "    imc_recons = model(stpt.double()).cuda() # throw away class predictions\n",
    "    loss = criterion(imc_recons.double(), imc.double())\n",
    "    losses.update(loss.item(), stpt.size(0))\n",
    "\n",
    "    # Record time to do forward passes and save images\n",
    "    batch_time.update(time.time() - end)\n",
    "    end = time.time()\n",
    "\n",
    "    # Print model accuracy -- in the code below, val refers to both value and validation\n",
    "    if i % 25 == 0:\n",
    "      print('Validate: [{0}/{1}]\\t'\n",
    "            'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "            'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
    "             i, len(val_loader), batch_time=batch_time, loss=losses))\n",
    "    \n",
    "  return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "whole-sweet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch, plot=True):\n",
    "  print('='*10, 'Starting training epoch {}'.format(epoch), '='*10)\n",
    "  model.train()\n",
    "  \n",
    "  # Prepare value counters and timers\n",
    "  batch_time, data_time, losses = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "\n",
    "  end = time.time()\n",
    "  for i, (stpt, imc) in enumerate(train_loader):\n",
    "    \n",
    "    # Use GPU if available\n",
    "    if use_gpu:\n",
    "        stpt, imc = stpt.cuda(), imc.cuda()\n",
    "\n",
    "    # Record time to load data (above)\n",
    "    data_time.update(time.time() - end)\n",
    "\n",
    "    # Run forward pass\n",
    "    imc_recons = model(stpt.double()).cuda()\n",
    "    print('successfully reconstruct')\n",
    "    loss = criterion(imc_recons.double(), imc.double()) \n",
    "    losses.update(loss.item(), stpt.size(0))\n",
    "\n",
    "    # Compute gradient and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Record time to do forward and backward passes\n",
    "    batch_time.update(time.time() - end)\n",
    "    end = time.time()\n",
    "\n",
    "    # Print model accuracy -- in the code below, val refers to value, not validation\n",
    "    if i % 25 == 0:\n",
    "      print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "            'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "            'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "            'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
    "              epoch, i, len(train_loader), batch_time=batch_time,\n",
    "             data_time=data_time, loss=losses)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "median-completion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Starting training epoch 0 ==========\n",
      "got through mlp\n",
      "sum\n",
      "weird thing\n",
      "second sum\n",
      "successfully reconstruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kyang/.local/lib/python3.7/site-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([1, 40, 256, 256])) that is different to the input size (torch.Size([40, 256, 256])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 17.09 GiB (GPU 0; 31.72 GiB total capacity; 18.55 GiB already allocated; 11.72 GiB free; 18.79 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c494f11e5376>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0minc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0;31m# Train for one epoch, then validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m       \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-d4986d1d7bb3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer, epoch, plot)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Compute gradient and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 17.09 GiB (GPU 0; 31.72 GiB total capacity; 18.55 GiB already allocated; 11.72 GiB free; 18.79 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    best_losses = 1e10\n",
    "    epochs = 20\n",
    "\n",
    "    # Train model\n",
    "    for epoch in range(epochs):\n",
    "      epoch += inc\n",
    "      # Train for one epoch, then validate\n",
    "      train(train_loader, model, criterion, optimizer, epoch)\n",
    "      with torch.no_grad():\n",
    "        losses = validate(val_loader, model, criterion, epoch)\n",
    "      # Save checkpoint and replace old best model if current model is better\n",
    "      if losses < best_losses:\n",
    "        best_losses = losses\n",
    "        torch.save({'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': losses,\n",
    "                    'epoch': epoch,\n",
    "                    'loss': losses\n",
    "                   }, '../checkpoints/pointsetgen/model-epoch-{}-losses-{:.3f}.pth'.format(epoch+1,losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-ministry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I WANT TO GET THIS WORKING!!!\n",
    "\n",
    "x = torch.randn(1, 64, 875, 3).double()\n",
    "uv = torch.meshgrid(torch.arange(0, 256), torch.arange(0, 256))\n",
    "uv = torch.stack(uv).permute(1,2,0).type(torch.uint8).cuda()  # [256, 256, 2]\n",
    "xy = torch.sum(x.squeeze().type(torch.float32), dim=0).cuda() # [875, 3]\n",
    "\n",
    "img = torch.exp(((uv[None,:,:,0]-xy[:,None,None,0])**2 + (uv[None,:,:,1]-xy[:,None,None,1])**2) / (xy[:,None,None,2]**2 + 1)).cuda()  # [875,256,256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-blackjack",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr = (uv[None,:,:,0]-xy[:,None,None,0])**2\n",
    "curr += (uv[None,:,:,1]-xy[:,None,None,1])**2\n",
    "curr /= (xy[:,None,None,2]**2 + 1)\n",
    "curr = torch.exp(curr)\n",
    "print(curr.shape)\n",
    "print(img_backup.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_backup = img.detach().clone()  # want to compare against this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-wednesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PointSetGen().double()\n",
    "x = torch.randn(1, 8, 256, 256).double()\n",
    "x = model(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-nerve",
   "metadata": {},
   "outputs": [],
   "source": [
    "xys = torch.split(xy, 10, dim=0)\n",
    "for i in range(len(xys)):\n",
    "    xys[i].cuda()\n",
    "\n",
    "img = None\n",
    "for i in range(len(xys)):\n",
    "    if img is None:\n",
    "        img = torch.exp(((uv[None,:,:,0]-xys[i][:,None,None,0])**2 + (uv[None,:,:,1]-xys[i][:,None,None,1])**2) / (xys[i][:,None,None,2]**2 + 1))\n",
    "    else:\n",
    "        img = torch.cat((img, torch.exp(((uv[None,:,:,0]-xys[i][:,None,None,0])**2 + (uv[None,:,:,1]-xys[i][:,None,None,1])**2) / (xys[i][:,None,None,2]**2 + 1))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpsc533y",
   "language": "python",
   "name": "cpsc533y"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
